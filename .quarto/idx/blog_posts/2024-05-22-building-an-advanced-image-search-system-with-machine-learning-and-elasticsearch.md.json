{"title":"Building an Advanced Image Search System with Machine Learning and ElasticSearch","markdown":{"yaml":{"title":"Building an Advanced Image Search System with Machine Learning and ElasticSearch","date":"2024-05-22","reading-time":"5 min read","categories":"computer-vision","image":"https://cdn-images-1.medium.com/max/1024/1*7PdAAwnYhSltU7dX7WkU_Q.jpeg"},"containsRefs":false,"markdown":"\n\n[click here to read this in medium](https://guttikondaparthasai.medium.com/building-an-advanced-image-search-system-with-machine-learning-and-elasticsearch-b2155ebbeada?source=rss-2c47946b91eb------2)\n\n<p>In today’s digital world, having a powerful image search system is invaluable. Imagine being able to search for images using other images rather than keywords. This article will guide you through building such an advanced image search system using machine learning techniques. We’ll leverage OpenAI’s CLIP model to process images into feature vectors and Elasticsearch (part of the ELK stack) to store and search these vectors using cosine similarity.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7PdAAwnYhSltU7dX7WkU_Q.jpeg\" /><figcaption>pic by me❤️</figcaption></figure><h3>Understanding Image Search with Machine Learning</h3><h4>What is Image Search?</h4><p>Image search is the process of finding images that are visually similar to a query image. Traditional image search engines rely on metadata or keywords associated with images. However, with advancements in machine learning, we can now search for images based on their visual content.</p><h4>How Does it Work?</h4><ol><li><strong>Feature Extraction</strong>: Transform images into feature vectors that capture their visual content.</li><li><strong>Indexing</strong>: Store these feature vectors in a database.</li><li><strong>Searching</strong>: Compare feature vectors using a similarity metric (e.g., cosine similarity) to find the most similar images.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*yCIPmedqQYK03N43\" /><figcaption>Photo by <a href=\"https://unsplash.com/@impatrickt?utm_source=medium&amp;utm_medium=referral\">Patrick Tomasso</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>Our Approach: Using CLIP and Elasticsearch</h3><h4>What is CLIP?</h4><p>CLIP (Contrastive Language–Image Pre-Training) is a model developed by OpenAI that can understand images and text in a unified manner. It can transform an image into a feature vector that encapsulates the image’s visual content.</p><h4>Why Elasticsearch?</h4><p>Elasticsearch is a powerful search engine that supports efficient storage and querying of large datasets. With the k-NN (k-nearest neighbors) feature, Elasticsearch can quickly find similar vectors, making it ideal for our image search system.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*IsK2pyDtmn5l-66A\" /><figcaption>Photo by <a href=\"https://unsplash.com/@kimsuzi08?utm_source=medium&amp;utm_medium=referral\">Suzi Kim</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>Step-by-Step Implementation</h3><h4>Step 1: Set Up the ELK Stack Using Docker</h4><p>First, we need to set up Elasticsearch and Kibana using Docker.<br />For setting up the ElasticSearch, Logstash and Kibana. Please clone this repo (<a href=\"https://github.com/propardhu/Docker_ELK_Image_Search\">https://github.com/propardhu/Docker_ELK_Image_Search</a>) and compose up the setup.<br /><strong>Verify the setup</strong>:<br />Elasticsearch: <a href=\"http://localhost:9200\">http://localhost:9200</a><br />Kibana: <a href=\"http://localhost:5601\">http://localhost:5601</a><br />username: elastic<br />password: changeme</p><h4>Step 2: Prepare and Index Images Using Python</h4><p>Next, we will prepare and index images into Elasticsearch using the Oxford Pets dataset.</p><h4>2.1. Install Dependencies</h4><pre>!pip install torch transformers pillow requests torchvision matplotlib</pre><h4>2.2. Download and Preprocess the Oxford Pets Dataset</h4><pre>import torch<br />from transformers import CLIPProcessor, CLIPModel<br />from PIL import Image<br />import requests<br />import json<br />import os<br />from torchvision import datasets, transforms<br />from torch.utils.data import DataLoader, Subset<br /><br /># Load the CLIP model and processor<br />model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)<br />processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)<br /># Function to preprocess images and extract features<br />def extract_features(image):<br />    inputs = processor(images=image, return_tensors=&quot;pt&quot;)<br />    with torch.no_grad():<br />        image_features = model.get_image_features(**inputs)<br />    image_features = image_features / image_features.norm(dim=-1, keepdim=True)<br />    return image_features.squeeze().tolist()<br /><br /># Directory to save the dataset<br />dataset_dir = &quot;./oxford_pets&quot;<br /><br /># Download and prepare the dataset (using Oxford Pets for demo)<br />transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])<br />dataset = datasets.OxfordIIITPet(root=dataset_dir, download=True, transform=transform)<br /><br /># Take a subset of 100 images<br />subset_indices = list(range(100))<br />subset = Subset(dataset, subset_indices)<br />data_loader = DataLoader(subset, batch_size=1, shuffle=False)<br /><br /># Ensure there are at least 100 images<br />assert len(subset) &gt;= 100, &quot;Dataset should contain at least 100 images.&quot;</pre><h4>2.3. Index Features in Elasticsearch</h4><pre># Elasticsearch settings<br />ES_HOST = &quot;http://localhost:9200&quot;<br />ES_INDEX = &quot;image-index&quot;<br />ES_USER = &quot;elastic&quot;<br />ES_PASS = &quot;changeme&quot;<br /><br />def index_image(image, label, image_id):<br />    features = extract_features(image)<br />    document = {<br />        &quot;name&quot;: f&quot;image_{image_id}&quot;,<br />        &quot;label&quot;: label,<br />        &quot;vector&quot;: features<br />    }<br />    response = requests.post(<br />        f&quot;{ES_HOST}/{ES_INDEX}/_doc/{image_id}&quot;,<br />        headers={&quot;Content-Type&quot;: &quot;application/json&quot;},<br />        auth=(ES_USER, ES_PASS),<br />        data=json.dumps(document)<br />    )<br />    return response.json()<br /><br /># Index the images with labels<br />for i, (image, label) in enumerate(data_loader):<br />    # Convert tensor to PIL image<br />    image = transforms.ToPILImage()(image[0])<br />    label = dataset.classes[label]<br />    result = index_image(image, label, i)<br />    image_path = os.path.join(dataset_dir, f&quot;image_{i}.jpg&quot;)<br />    image.save(image_path)  # Save the image for later retrieval<br />    print(f&quot;Indexed image {i} with label '{label}': {result}&quot;)</pre><h4>Step 3: Perform Image-to-Image Search and Display Images</h4><h4>3.1. Search for Similar Images</h4><pre>import matplotlib.pyplot as plt<br /><br /># Function to search for similar images<br />def search_similar_images(query_image_path, k=5):<br />    query_image = Image.open(query_image_path)<br />    query_features = extract_features(query_image)<br />    search_query = {<br />        &quot;knn&quot;: {<br />            &quot;field&quot;: &quot;vector&quot;,<br />            &quot;query_vector&quot;: query_features,<br />            &quot;k&quot;: k,<br />            &quot;num_candidates&quot;: 100<br />        }<br />    }<br />    response = requests.post(<br />        f&quot;{ES_HOST}/{ES_INDEX}/_knn_search&quot;,<br />        headers={&quot;Content-Type&quot;: &quot;application/json&quot;},<br />        auth=(ES_USER, ES_PASS),<br />        data=json.dumps(search_query)<br />    )<br />    return response.json()<br /><br /><br /># Function to display images<br />def display_images(query_image_path, search_results):<br />    query_image = Image.open(query_image_path)<br />    fig, axes = plt.subplots(1, 6, figsize=(20, 5))<br /><br />    # Display the query image<br />    axes[0].imshow(query_image)<br />    axes[0].set_title(&quot;Query Image&quot;)<br />    axes[0].axis('off')<br /><br />    # Display the top 5 similar images<br />    for i, hit in enumerate(search_results['hits']['hits']):<br />        image_id = hit['_id']<br />        label = hit['_source']['label']<br />        similar_image_path = os.path.join(dataset_dir, f&quot;image_{image_id}.jpg&quot;)<br />        <br />        similar_image = Image.open(similar_image_path)<br />        axes[i + 1].imshow(similar_image)<br />        axes[i + 1].set_title(f&quot;Label: {label}\\nScore: {hit['_score']:.2f}&quot;)<br />        axes[i + 1].axis('off')<br /><br />    plt.show()<br /><br /># Example search with a query image from the dataset<br />from random import randint<br />query_image, _ = subset[randint(1, 100)]<br />query_image = transforms.ToPILImage()(query_image)  # Convert tensor to PIL image<br />query_image_path = &quot;./query_image.jpg&quot;  # Save the query image to this path<br />query_image.save(query_image_path)  # Save the PIL image to the specified path<br /><br />query_result = search_similar_images(query_image_path)<br /><br /># Display the results<br />display_images(query_image_path, query_result)</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FcvVNcBmUI7bOF9VNuMx_g.png\" /><figcaption>Sample result-1</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GpX54EQZPz1_N-HPwVczCg.png\" /><figcaption>Sample result-2</figcaption></figure><p>Note: I have created An flask app to show the search results.</p><p>All the code is available at</p><p><a href=\"https://github.com/propardhu/Docker_ELK_Image_Search/tree/main\">GitHub - propardhu/Docker_ELK_Image_Search</a></p><h3>Conclusion</h3><p>In this article, we built an advanced image search system using OpenAI’s CLIP model and Elasticsearch. By setting up the ELK stack with Docker, downloading and preparing the Oxford Pets dataset, indexing image features into Elasticsearch, and performing image.</p><h4>Thank you..!</h4><p><a href=\"https://guttikondaparthasai.medium.com/\">Pardhu Guttikonda - Medium</a></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b2155ebbeada\" width=\"1\" />\n","srcMarkdownNoYaml":"\n\n[click here to read this in medium](https://guttikondaparthasai.medium.com/building-an-advanced-image-search-system-with-machine-learning-and-elasticsearch-b2155ebbeada?source=rss-2c47946b91eb------2)\n\n<p>In today’s digital world, having a powerful image search system is invaluable. Imagine being able to search for images using other images rather than keywords. This article will guide you through building such an advanced image search system using machine learning techniques. We’ll leverage OpenAI’s CLIP model to process images into feature vectors and Elasticsearch (part of the ELK stack) to store and search these vectors using cosine similarity.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*7PdAAwnYhSltU7dX7WkU_Q.jpeg\" /><figcaption>pic by me❤️</figcaption></figure><h3>Understanding Image Search with Machine Learning</h3><h4>What is Image Search?</h4><p>Image search is the process of finding images that are visually similar to a query image. Traditional image search engines rely on metadata or keywords associated with images. However, with advancements in machine learning, we can now search for images based on their visual content.</p><h4>How Does it Work?</h4><ol><li><strong>Feature Extraction</strong>: Transform images into feature vectors that capture their visual content.</li><li><strong>Indexing</strong>: Store these feature vectors in a database.</li><li><strong>Searching</strong>: Compare feature vectors using a similarity metric (e.g., cosine similarity) to find the most similar images.</li></ol><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*yCIPmedqQYK03N43\" /><figcaption>Photo by <a href=\"https://unsplash.com/@impatrickt?utm_source=medium&amp;utm_medium=referral\">Patrick Tomasso</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>Our Approach: Using CLIP and Elasticsearch</h3><h4>What is CLIP?</h4><p>CLIP (Contrastive Language–Image Pre-Training) is a model developed by OpenAI that can understand images and text in a unified manner. It can transform an image into a feature vector that encapsulates the image’s visual content.</p><h4>Why Elasticsearch?</h4><p>Elasticsearch is a powerful search engine that supports efficient storage and querying of large datasets. With the k-NN (k-nearest neighbors) feature, Elasticsearch can quickly find similar vectors, making it ideal for our image search system.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*IsK2pyDtmn5l-66A\" /><figcaption>Photo by <a href=\"https://unsplash.com/@kimsuzi08?utm_source=medium&amp;utm_medium=referral\">Suzi Kim</a> on <a href=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><h3>Step-by-Step Implementation</h3><h4>Step 1: Set Up the ELK Stack Using Docker</h4><p>First, we need to set up Elasticsearch and Kibana using Docker.<br />For setting up the ElasticSearch, Logstash and Kibana. Please clone this repo (<a href=\"https://github.com/propardhu/Docker_ELK_Image_Search\">https://github.com/propardhu/Docker_ELK_Image_Search</a>) and compose up the setup.<br /><strong>Verify the setup</strong>:<br />Elasticsearch: <a href=\"http://localhost:9200\">http://localhost:9200</a><br />Kibana: <a href=\"http://localhost:5601\">http://localhost:5601</a><br />username: elastic<br />password: changeme</p><h4>Step 2: Prepare and Index Images Using Python</h4><p>Next, we will prepare and index images into Elasticsearch using the Oxford Pets dataset.</p><h4>2.1. Install Dependencies</h4><pre>!pip install torch transformers pillow requests torchvision matplotlib</pre><h4>2.2. Download and Preprocess the Oxford Pets Dataset</h4><pre>import torch<br />from transformers import CLIPProcessor, CLIPModel<br />from PIL import Image<br />import requests<br />import json<br />import os<br />from torchvision import datasets, transforms<br />from torch.utils.data import DataLoader, Subset<br /><br /># Load the CLIP model and processor<br />model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)<br />processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)<br /># Function to preprocess images and extract features<br />def extract_features(image):<br />    inputs = processor(images=image, return_tensors=&quot;pt&quot;)<br />    with torch.no_grad():<br />        image_features = model.get_image_features(**inputs)<br />    image_features = image_features / image_features.norm(dim=-1, keepdim=True)<br />    return image_features.squeeze().tolist()<br /><br /># Directory to save the dataset<br />dataset_dir = &quot;./oxford_pets&quot;<br /><br /># Download and prepare the dataset (using Oxford Pets for demo)<br />transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])<br />dataset = datasets.OxfordIIITPet(root=dataset_dir, download=True, transform=transform)<br /><br /># Take a subset of 100 images<br />subset_indices = list(range(100))<br />subset = Subset(dataset, subset_indices)<br />data_loader = DataLoader(subset, batch_size=1, shuffle=False)<br /><br /># Ensure there are at least 100 images<br />assert len(subset) &gt;= 100, &quot;Dataset should contain at least 100 images.&quot;</pre><h4>2.3. Index Features in Elasticsearch</h4><pre># Elasticsearch settings<br />ES_HOST = &quot;http://localhost:9200&quot;<br />ES_INDEX = &quot;image-index&quot;<br />ES_USER = &quot;elastic&quot;<br />ES_PASS = &quot;changeme&quot;<br /><br />def index_image(image, label, image_id):<br />    features = extract_features(image)<br />    document = {<br />        &quot;name&quot;: f&quot;image_{image_id}&quot;,<br />        &quot;label&quot;: label,<br />        &quot;vector&quot;: features<br />    }<br />    response = requests.post(<br />        f&quot;{ES_HOST}/{ES_INDEX}/_doc/{image_id}&quot;,<br />        headers={&quot;Content-Type&quot;: &quot;application/json&quot;},<br />        auth=(ES_USER, ES_PASS),<br />        data=json.dumps(document)<br />    )<br />    return response.json()<br /><br /># Index the images with labels<br />for i, (image, label) in enumerate(data_loader):<br />    # Convert tensor to PIL image<br />    image = transforms.ToPILImage()(image[0])<br />    label = dataset.classes[label]<br />    result = index_image(image, label, i)<br />    image_path = os.path.join(dataset_dir, f&quot;image_{i}.jpg&quot;)<br />    image.save(image_path)  # Save the image for later retrieval<br />    print(f&quot;Indexed image {i} with label '{label}': {result}&quot;)</pre><h4>Step 3: Perform Image-to-Image Search and Display Images</h4><h4>3.1. Search for Similar Images</h4><pre>import matplotlib.pyplot as plt<br /><br /># Function to search for similar images<br />def search_similar_images(query_image_path, k=5):<br />    query_image = Image.open(query_image_path)<br />    query_features = extract_features(query_image)<br />    search_query = {<br />        &quot;knn&quot;: {<br />            &quot;field&quot;: &quot;vector&quot;,<br />            &quot;query_vector&quot;: query_features,<br />            &quot;k&quot;: k,<br />            &quot;num_candidates&quot;: 100<br />        }<br />    }<br />    response = requests.post(<br />        f&quot;{ES_HOST}/{ES_INDEX}/_knn_search&quot;,<br />        headers={&quot;Content-Type&quot;: &quot;application/json&quot;},<br />        auth=(ES_USER, ES_PASS),<br />        data=json.dumps(search_query)<br />    )<br />    return response.json()<br /><br /><br /># Function to display images<br />def display_images(query_image_path, search_results):<br />    query_image = Image.open(query_image_path)<br />    fig, axes = plt.subplots(1, 6, figsize=(20, 5))<br /><br />    # Display the query image<br />    axes[0].imshow(query_image)<br />    axes[0].set_title(&quot;Query Image&quot;)<br />    axes[0].axis('off')<br /><br />    # Display the top 5 similar images<br />    for i, hit in enumerate(search_results['hits']['hits']):<br />        image_id = hit['_id']<br />        label = hit['_source']['label']<br />        similar_image_path = os.path.join(dataset_dir, f&quot;image_{image_id}.jpg&quot;)<br />        <br />        similar_image = Image.open(similar_image_path)<br />        axes[i + 1].imshow(similar_image)<br />        axes[i + 1].set_title(f&quot;Label: {label}\\nScore: {hit['_score']:.2f}&quot;)<br />        axes[i + 1].axis('off')<br /><br />    plt.show()<br /><br /># Example search with a query image from the dataset<br />from random import randint<br />query_image, _ = subset[randint(1, 100)]<br />query_image = transforms.ToPILImage()(query_image)  # Convert tensor to PIL image<br />query_image_path = &quot;./query_image.jpg&quot;  # Save the query image to this path<br />query_image.save(query_image_path)  # Save the PIL image to the specified path<br /><br />query_result = search_similar_images(query_image_path)<br /><br /># Display the results<br />display_images(query_image_path, query_result)</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FcvVNcBmUI7bOF9VNuMx_g.png\" /><figcaption>Sample result-1</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GpX54EQZPz1_N-HPwVczCg.png\" /><figcaption>Sample result-2</figcaption></figure><p>Note: I have created An flask app to show the search results.</p><p>All the code is available at</p><p><a href=\"https://github.com/propardhu/Docker_ELK_Image_Search/tree/main\">GitHub - propardhu/Docker_ELK_Image_Search</a></p><h3>Conclusion</h3><p>In this article, we built an advanced image search system using OpenAI’s CLIP model and Elasticsearch. By setting up the ELK stack with Docker, downloading and preparing the Oxford Pets dataset, indexing image features into Elasticsearch, and performing image.</p><h4>Thank you..!</h4><p><a href=\"https://guttikondaparthasai.medium.com/\">Pardhu Guttikonda - Medium</a></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b2155ebbeada\" width=\"1\" />\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"2024-05-22-building-an-advanced-image-search-system-with-machine-learning-and-elasticsearch.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.56","theme":{"light":["simplex","styles.scss"],"dark":["superhero","styles.scss"]},"toc-location":"left","title":"Building an Advanced Image Search System with Machine Learning and ElasticSearch","date":"2024-05-22","reading-time":"5 min read","categories":"computer-vision","image":"https://cdn-images-1.medium.com/max/1024/1*7PdAAwnYhSltU7dX7WkU_Q.jpeg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}