{'title': 'Building an Advanced Image Search System with Machine Learning and ElasticSearch', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://medium.com/feed/@guttikondaparthasai', 'value': 'Building an Advanced Image Search System with Machine Learning and ElasticSearch'}, 'links': [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://guttikondaparthasai.medium.com/building-an-advanced-image-search-system-with-machine-learning-and-elasticsearch-b2155ebbeada?source=rss-2c47946b91eb------2'}], 'link': 'https://guttikondaparthasai.medium.com/building-an-advanced-image-search-system-with-machine-learning-and-elasticsearch-b2155ebbeada?source=rss-2c47946b91eb------2', 'id': 'https://medium.com/p/b2155ebbeada', 'guidislink': False, 'tags': [{'term': 'computer-vision', 'scheme': None, 'label': None}, {'term': 'elasticsearch', 'scheme': None, 'label': None}, {'term': 'openai', 'scheme': None, 'label': None}, {'term': 'image-processing', 'scheme': None, 'label': None}, {'term': 'machine-learning', 'scheme': None, 'label': None}], 'authors': [{'name': 'Pardhu Guttikonda'}], 'author': 'Pardhu Guttikonda', 'author_detail': {'name': 'Pardhu Guttikonda'}, 'published': 'Wed, 22 May 2024 21:18:49 GMT', 'published_parsed': time.struct_time(tm_year=2024, tm_mon=5, tm_mday=22, tm_hour=21, tm_min=18, tm_sec=49, tm_wday=2, tm_yday=143, tm_isdst=0), 'updated': '2024-05-22T21:18:49.620Z', 'updated_parsed': time.struct_time(tm_year=2024, tm_mon=5, tm_mday=22, tm_hour=21, tm_min=18, tm_sec=49, tm_wday=2, tm_yday=143, tm_isdst=0), 'content': [{'type': 'text/html', 'language': None, 'base': 'https://medium.com/feed/@guttikondaparthasai', 'value': '<p>In today’s digital world, having a powerful image search system is invaluable. Imagine being able to search for images using other images rather than keywords. This article will guide you through building such an advanced image search system using machine learning techniques. We’ll leverage OpenAI’s CLIP model to process images into feature vectors and Elasticsearch (part of the ELK stack) to store and search these vectors using cosine similarity.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PdAAwnYhSltU7dX7WkU_Q.jpeg" /><figcaption>pic by\xa0me❤️</figcaption></figure><h3>Understanding Image Search with Machine\xa0Learning</h3><h4>What is Image\xa0Search?</h4><p>Image search is the process of finding images that are visually similar to a query image. Traditional image search engines rely on metadata or keywords associated with images. However, with advancements in machine learning, we can now search for images based on their visual\xa0content.</p><h4>How Does it\xa0Work?</h4><ol><li><strong>Feature Extraction</strong>: Transform images into feature vectors that capture their visual\xa0content.</li><li><strong>Indexing</strong>: Store these feature vectors in a database.</li><li><strong>Searching</strong>: Compare feature vectors using a similarity metric (e.g., cosine similarity) to find the most similar\xa0images.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yCIPmedqQYK03N43" /><figcaption>Photo by <a href="https://unsplash.com/@impatrickt?utm_source=medium&amp;utm_medium=referral">Patrick Tomasso</a> on\xa0<a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><h3>Our Approach: Using CLIP and Elasticsearch</h3><h4>What is\xa0CLIP?</h4><p>CLIP (Contrastive Language–Image Pre-Training) is a model developed by OpenAI that can understand images and text in a unified manner. It can transform an image into a feature vector that encapsulates the image’s visual\xa0content.</p><h4>Why Elasticsearch?</h4><p>Elasticsearch is a powerful search engine that supports efficient storage and querying of large datasets. With the k-NN (k-nearest neighbors) feature, Elasticsearch can quickly find similar vectors, making it ideal for our image search\xa0system.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IsK2pyDtmn5l-66A" /><figcaption>Photo by <a href="https://unsplash.com/@kimsuzi08?utm_source=medium&amp;utm_medium=referral">Suzi Kim</a> on\xa0<a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><h3>Step-by-Step Implementation</h3><h4>Step 1: Set Up the ELK Stack Using\xa0Docker</h4><p>First, we need to set up Elasticsearch and Kibana using Docker.<br />For setting up the ElasticSearch, Logstash and Kibana. Please clone this repo (<a href="https://github.com/propardhu/Docker_ELK_Image_Search">https://github.com/propardhu/Docker_ELK_Image_Search</a>) and compose up the setup.<br /><strong>Verify the setup</strong>:<br />Elasticsearch: <a href="http://localhost:9200">http://localhost:9200</a><br />Kibana: <a href="http://localhost:5601">http://localhost:5601</a><br />username: elastic<br />password: changeme</p><h4>Step 2: Prepare and Index Images Using\xa0Python</h4><p>Next, we will prepare and index images into Elasticsearch using the Oxford Pets\xa0dataset.</p><h4>2.1. Install Dependencies</h4><pre>!pip install torch transformers pillow requests torchvision matplotlib</pre><h4>2.2. Download and Preprocess the Oxford Pets\xa0Dataset</h4><pre>import torch<br />from transformers import CLIPProcessor, CLIPModel<br />from PIL import Image<br />import requests<br />import json<br />import os<br />from torchvision import datasets, transforms<br />from torch.utils.data import DataLoader, Subset<br /><br /># Load the CLIP model and processor<br />model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)<br />processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)<br /># Function to preprocess images and extract features<br />def extract_features(image):<br />    inputs = processor(images=image, return_tensors=&quot;pt&quot;)<br />    with torch.no_grad():<br />        image_features = model.get_image_features(**inputs)<br />    image_features = image_features / image_features.norm(dim=-1, keepdim=True)<br />    return image_features.squeeze().tolist()<br /><br /># Directory to save the dataset<br />dataset_dir = &quot;./oxford_pets&quot;<br /><br /># Download and prepare the dataset (using Oxford Pets for demo)<br />transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])<br />dataset = datasets.OxfordIIITPet(root=dataset_dir, download=True, transform=transform)<br /><br /># Take a subset of 100 images<br />subset_indices = list(range(100))<br />subset = Subset(dataset, subset_indices)<br />data_loader = DataLoader(subset, batch_size=1, shuffle=False)<br /><br /># Ensure there are at least 100 images<br />assert len(subset) &gt;= 100, &quot;Dataset should contain at least 100 images.&quot;</pre><h4>2.3. Index Features in Elasticsearch</h4><pre># Elasticsearch settings<br />ES_HOST = &quot;http://localhost:9200&quot;<br />ES_INDEX = &quot;image-index&quot;<br />ES_USER = &quot;elastic&quot;<br />ES_PASS = &quot;changeme&quot;<br /><br />def index_image(image, label, image_id):<br />    features = extract_features(image)<br />    document = {<br />        &quot;name&quot;: f&quot;image_{image_id}&quot;,<br />        &quot;label&quot;: label,<br />        &quot;vector&quot;: features<br />    }<br />    response = requests.post(<br />        f&quot;{ES_HOST}/{ES_INDEX}/_doc/{image_id}&quot;,<br />        headers={&quot;Content-Type&quot;: &quot;application/json&quot;},<br />        auth=(ES_USER, ES_PASS),<br />        data=json.dumps(document)<br />    )<br />    return response.json()<br /><br /># Index the images with labels<br />for i, (image, label) in enumerate(data_loader):<br />    # Convert tensor to PIL image<br />    image = transforms.ToPILImage()(image[0])<br />    label = dataset.classes[label]<br />    result = index_image(image, label, i)<br />    image_path = os.path.join(dataset_dir, f&quot;image_{i}.jpg&quot;)<br />    image.save(image_path)  # Save the image for later retrieval<br />    print(f&quot;Indexed image {i} with label \'{label}\': {result}&quot;)</pre><h4>Step 3: Perform Image-to-Image Search and Display\xa0Images</h4><h4>3.1. Search for Similar\xa0Images</h4><pre>import matplotlib.pyplot as plt<br /><br /># Function to search for similar images<br />def search_similar_images(query_image_path, k=5):<br />    query_image = Image.open(query_image_path)<br />    query_features = extract_features(query_image)<br />    search_query = {<br />        &quot;knn&quot;: {<br />            &quot;field&quot;: &quot;vector&quot;,<br />            &quot;query_vector&quot;: query_features,<br />            &quot;k&quot;: k,<br />            &quot;num_candidates&quot;: 100<br />        }<br />    }<br />    response = requests.post(<br />        f&quot;{ES_HOST}/{ES_INDEX}/_knn_search&quot;,<br />        headers={&quot;Content-Type&quot;: &quot;application/json&quot;},<br />        auth=(ES_USER, ES_PASS),<br />        data=json.dumps(search_query)<br />    )<br />    return response.json()<br /><br /><br /># Function to display images<br />def display_images(query_image_path, search_results):<br />    query_image = Image.open(query_image_path)<br />    fig, axes = plt.subplots(1, 6, figsize=(20, 5))<br /><br />    # Display the query image<br />    axes[0].imshow(query_image)<br />    axes[0].set_title(&quot;Query Image&quot;)<br />    axes[0].axis(\'off\')<br /><br />    # Display the top 5 similar images<br />    for i, hit in enumerate(search_results[\'hits\'][\'hits\']):<br />        image_id = hit[\'_id\']<br />        label = hit[\'_source\'][\'label\']<br />        similar_image_path = os.path.join(dataset_dir, f&quot;image_{image_id}.jpg&quot;)<br />        <br />        similar_image = Image.open(similar_image_path)<br />        axes[i + 1].imshow(similar_image)<br />        axes[i + 1].set_title(f&quot;Label: {label}\\nScore: {hit[\'_score\']:.2f}&quot;)<br />        axes[i + 1].axis(\'off\')<br /><br />    plt.show()<br /><br /># Example search with a query image from the dataset<br />from random import randint<br />query_image, _ = subset[randint(1, 100)]<br />query_image = transforms.ToPILImage()(query_image)  # Convert tensor to PIL image<br />query_image_path = &quot;./query_image.jpg&quot;  # Save the query image to this path<br />query_image.save(query_image_path)  # Save the PIL image to the specified path<br /><br />query_result = search_similar_images(query_image_path)<br /><br /># Display the results<br />display_images(query_image_path, query_result)</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FcvVNcBmUI7bOF9VNuMx_g.png" /><figcaption>Sample result-1</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GpX54EQZPz1_N-HPwVczCg.png" /><figcaption>Sample result-2</figcaption></figure><p>Note: I have created An flask app to show the search\xa0results.</p><p>All the code is available at</p><p><a href="https://github.com/propardhu/Docker_ELK_Image_Search/tree/main">GitHub - propardhu/Docker_ELK_Image_Search</a></p><h3>Conclusion</h3><p>In this article, we built an advanced image search system using OpenAI’s CLIP model and Elasticsearch. By setting up the ELK stack with Docker, downloading and preparing the Oxford Pets dataset, indexing image features into Elasticsearch, and performing image.</p><h4>Thank you..!</h4><p><a href="https://guttikondaparthasai.medium.com/">Pardhu Guttikonda - Medium</a></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b2155ebbeada" width="1" />'}], 'summary': '<p>In today’s digital world, having a powerful image search system is invaluable. Imagine being able to search for images using other images rather than keywords. This article will guide you through building such an advanced image search system using machine learning techniques. We’ll leverage OpenAI’s CLIP model to process images into feature vectors and Elasticsearch (part of the ELK stack) to store and search these vectors using cosine similarity.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PdAAwnYhSltU7dX7WkU_Q.jpeg" /><figcaption>pic by\xa0me❤️</figcaption></figure><h3>Understanding Image Search with Machine\xa0Learning</h3><h4>What is Image\xa0Search?</h4><p>Image search is the process of finding images that are visually similar to a query image. Traditional image search engines rely on metadata or keywords associated with images. However, with advancements in machine learning, we can now search for images based on their visual\xa0content.</p><h4>How Does it\xa0Work?</h4><ol><li><strong>Feature Extraction</strong>: Transform images into feature vectors that capture their visual\xa0content.</li><li><strong>Indexing</strong>: Store these feature vectors in a database.</li><li><strong>Searching</strong>: Compare feature vectors using a similarity metric (e.g., cosine similarity) to find the most similar\xa0images.</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*yCIPmedqQYK03N43" /><figcaption>Photo by <a href="https://unsplash.com/@impatrickt?utm_source=medium&amp;utm_medium=referral">Patrick Tomasso</a> on\xa0<a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><h3>Our Approach: Using CLIP and Elasticsearch</h3><h4>What is\xa0CLIP?</h4><p>CLIP (Contrastive Language–Image Pre-Training) is a model developed by OpenAI that can understand images and text in a unified manner. It can transform an image into a feature vector that encapsulates the image’s visual\xa0content.</p><h4>Why Elasticsearch?</h4><p>Elasticsearch is a powerful search engine that supports efficient storage and querying of large datasets. With the k-NN (k-nearest neighbors) feature, Elasticsearch can quickly find similar vectors, making it ideal for our image search\xa0system.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IsK2pyDtmn5l-66A" /><figcaption>Photo by <a href="https://unsplash.com/@kimsuzi08?utm_source=medium&amp;utm_medium=referral">Suzi Kim</a> on\xa0<a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><h3>Step-by-Step Implementation</h3><h4>Step 1: Set Up the ELK Stack Using\xa0Docker</h4><p>First, we need to set up Elasticsearch and Kibana using Docker.<br />For setting up the ElasticSearch, Logstash and Kibana. Please clone this repo (<a href="https://github.com/propardhu/Docker_ELK_Image_Search">https://github.com/propardhu/Docker_ELK_Image_Search</a>) and compose up the setup.<br /><strong>Verify the setup</strong>:<br />Elasticsearch: <a href="http://localhost:9200">http://localhost:9200</a><br />Kibana: <a href="http://localhost:5601">http://localhost:5601</a><br />username: elastic<br />password: changeme</p><h4>Step 2: Prepare and Index Images Using\xa0Python</h4><p>Next, we will prepare and index images into Elasticsearch using the Oxford Pets\xa0dataset.</p><h4>2.1. Install Dependencies</h4><pre>!pip install torch transformers pillow requests torchvision matplotlib</pre><h4>2.2. Download and Preprocess the Oxford Pets\xa0Dataset</h4><pre>import torch<br />from transformers import CLIPProcessor, CLIPModel<br />from PIL import Image<br />import requests<br />import json<br />import os<br />from torchvision import datasets, transforms<br />from torch.utils.data import DataLoader, Subset<br /><br /># Load the CLIP model and processor<br />model = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)<br />processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)<br /># Function to preprocess images and extract features<br />def extract_features(image):<br />    inputs = processor(images=image, return_tensors=&quot;pt&quot;)<br />    with torch.no_grad():<br />        image_features = model.get_image_features(**inputs)<br />    image_features = image_features / image_features.norm(dim=-1, keepdim=True)<br />    return image_features.squeeze().tolist()<br /><br /># Directory to save the dataset<br />dataset_dir = &quot;./oxford_pets&quot;<br /><br /># Download and prepare the dataset (using Oxford Pets for demo)<br />transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])<br />dataset = datasets.OxfordIIITPet(root=dataset_dir, download=True, transform=transform)<br /><br /># Take a subset of 100 images<br />subset_indices = list(range(100))<br />subset = Subset(dataset, subset_indices)<br />data_loader = DataLoader(subset, batch_size=1, shuffle=False)<br /><br /># Ensure there are at least 100 images<br />assert len(subset) &gt;= 100, &quot;Dataset should contain at least 100 images.&quot;</pre><h4>2.3. Index Features in Elasticsearch</h4><pre># Elasticsearch settings<br />ES_HOST = &quot;http://localhost:9200&quot;<br />ES_INDEX = &quot;image-index&quot;<br />ES_USER = &quot;elastic&quot;<br />ES_PASS = &quot;changeme&quot;<br /><br />def index_image(image, label, image_id):<br />    features = extract_features(image)<br />    document = {<br />        &quot;name&quot;: f&quot;image_{image_id}&quot;,<br />        &quot;label&quot;: label,<br />        &quot;vector&quot;: features<br />    }<br />    response = requests.post(<br />        f&quot;{ES_HOST}/{ES_INDEX}/_doc/{image_id}&quot;,<br />        headers={&quot;Content-Type&quot;: &quot;application/json&quot;},<br />        auth=(ES_USER, ES_PASS),<br />        data=json.dumps(document)<br />    )<br />    return response.json()<br /><br /># Index the images with labels<br />for i, (image, label) in enumerate(data_loader):<br />    # Convert tensor to PIL image<br />    image = transforms.ToPILImage()(image[0])<br />    label = dataset.classes[label]<br />    result = index_image(image, label, i)<br />    image_path = os.path.join(dataset_dir, f&quot;image_{i}.jpg&quot;)<br />    image.save(image_path)  # Save the image for later retrieval<br />    print(f&quot;Indexed image {i} with label \'{label}\': {result}&quot;)</pre><h4>Step 3: Perform Image-to-Image Search and Display\xa0Images</h4><h4>3.1. Search for Similar\xa0Images</h4><pre>import matplotlib.pyplot as plt<br /><br /># Function to search for similar images<br />def search_similar_images(query_image_path, k=5):<br />    query_image = Image.open(query_image_path)<br />    query_features = extract_features(query_image)<br />    search_query = {<br />        &quot;knn&quot;: {<br />            &quot;field&quot;: &quot;vector&quot;,<br />            &quot;query_vector&quot;: query_features,<br />            &quot;k&quot;: k,<br />            &quot;num_candidates&quot;: 100<br />        }<br />    }<br />    response = requests.post(<br />        f&quot;{ES_HOST}/{ES_INDEX}/_knn_search&quot;,<br />        headers={&quot;Content-Type&quot;: &quot;application/json&quot;},<br />        auth=(ES_USER, ES_PASS),<br />        data=json.dumps(search_query)<br />    )<br />    return response.json()<br /><br /><br /># Function to display images<br />def display_images(query_image_path, search_results):<br />    query_image = Image.open(query_image_path)<br />    fig, axes = plt.subplots(1, 6, figsize=(20, 5))<br /><br />    # Display the query image<br />    axes[0].imshow(query_image)<br />    axes[0].set_title(&quot;Query Image&quot;)<br />    axes[0].axis(\'off\')<br /><br />    # Display the top 5 similar images<br />    for i, hit in enumerate(search_results[\'hits\'][\'hits\']):<br />        image_id = hit[\'_id\']<br />        label = hit[\'_source\'][\'label\']<br />        similar_image_path = os.path.join(dataset_dir, f&quot;image_{image_id}.jpg&quot;)<br />        <br />        similar_image = Image.open(similar_image_path)<br />        axes[i + 1].imshow(similar_image)<br />        axes[i + 1].set_title(f&quot;Label: {label}\\nScore: {hit[\'_score\']:.2f}&quot;)<br />        axes[i + 1].axis(\'off\')<br /><br />    plt.show()<br /><br /># Example search with a query image from the dataset<br />from random import randint<br />query_image, _ = subset[randint(1, 100)]<br />query_image = transforms.ToPILImage()(query_image)  # Convert tensor to PIL image<br />query_image_path = &quot;./query_image.jpg&quot;  # Save the query image to this path<br />query_image.save(query_image_path)  # Save the PIL image to the specified path<br /><br />query_result = search_similar_images(query_image_path)<br /><br /># Display the results<br />display_images(query_image_path, query_result)</pre><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*FcvVNcBmUI7bOF9VNuMx_g.png" /><figcaption>Sample result-1</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GpX54EQZPz1_N-HPwVczCg.png" /><figcaption>Sample result-2</figcaption></figure><p>Note: I have created An flask app to show the search\xa0results.</p><p>All the code is available at</p><p><a href="https://github.com/propardhu/Docker_ELK_Image_Search/tree/main">GitHub - propardhu/Docker_ELK_Image_Search</a></p><h3>Conclusion</h3><p>In this article, we built an advanced image search system using OpenAI’s CLIP model and Elasticsearch. By setting up the ELK stack with Docker, downloading and preparing the Oxford Pets dataset, indexing image features into Elasticsearch, and performing image.</p><h4>Thank you..!</h4><p><a href="https://guttikondaparthasai.medium.com/">Pardhu Guttikonda - Medium</a></p><img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b2155ebbeada" width="1" />'}